{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_TEMPERATURE = float(os.getenv(\"OPENAI_TEMPERATURE\", 0.0))\n",
    "\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_json('/Users/kim-yeongsang/Desktop/gme/instructino_ai/data_cbnu_rag_question.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any', axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm_pandas(tqdm())\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "df['question_vector'] = df.question.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vector_id\"] = range(0, len(df))\n",
    "df = df.astype({'vector_id':'str'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('data_rag_question_embedding_.json', force_ascii=False, orient = 'records',indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# Test that your OpenAI API key is correctly set as an environment variable\n",
    "# Note. if you run this notebook locally, you will need to reload your terminal and the notebook for the env variables to be live.\n",
    "\n",
    "# Note. alternatively you can set a temporary env variable like this:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print (\"OPENAI_API_KEY is ready\")\n",
    "else:\n",
    "    print (\"OPENAI_API_KEY environment variable not found\")\n",
    "\n",
    "\n",
    "embedding_function = OpenAIEmbeddingFunction(api_key=os.environ.get('OPENAI_API_KEY'), model_name='text-embedding-ada-002')\n",
    "question_collection = chroma_client.create_collection(name='quesiton', embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 166\n",
    "chunks = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "for chunk in chunks:\n",
    "    question_collection.add(\n",
    "        ids=chunk['vector_id'].tolist(),\n",
    "        embeddings=chunk['question_vector'].tolist(),  # Assuming you have the 'question_vector' column\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_collection(collection, query, max_results, dataframe):\n",
    "    results = collection.query(query_texts=query, n_results=max_results, include=['distances']) \n",
    "    df = pd.DataFrame({\n",
    "                'id':results['ids'][0], \n",
    "                'score':results['distances'][0],\n",
    "                'fileName': dataframe[dataframe.vector_id.isin(results['ids'][0])]['fileName'],\n",
    "                'content': dataframe[dataframe.vector_id.isin(results['ids'][0])]['content'],\n",
    "                'question': dataframe[dataframe.vector_id.isin(results['ids'][0])]['question'],\n",
    "                })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", os.getenv(\"OPENAI_API_MODEL\", \"gpt-3.5-turbo\")).lower()\n",
    "OPENAI_TEMPERATURE = float(os.getenv(\"OPENAI_TEMPERATURE\", 0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken as tiktoken\n",
    "import time\n",
    "\n",
    "\n",
    "def limit_tokens_from_string(string: str, model: str, limit: int) -> str:\n",
    "    \"\"\"Limits the string to a number of tokens (estimated).\"\"\"\n",
    "\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except:\n",
    "        encoding = tiktoken.encoding_for_model('gpt2')  # Fallback for others.\n",
    "\n",
    "    encoded = encoding.encode(string)\n",
    "\n",
    "    return encoding.decode(encoded[:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_call(\n",
    "    system_prompt: str,\n",
    "    prompt: str,\n",
    "    model: str = LLM_MODEL,\n",
    "    temperature: float = OPENAI_TEMPERATURE,\n",
    "    max_tokens: int = 100,\n",
    "):\n",
    "    while True:\n",
    "        try:\n",
    "            if not model.lower().startswith(\"gpt-\"):\n",
    "                # Use completion API\n",
    "                response = openai.Completion.create(\n",
    "                    engine=model,\n",
    "                    prompt=prompt,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    top_p=1,\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                )\n",
    "                return response.choices[0].text.strip()\n",
    "            else:\n",
    "                # Use 4000 instead of the real limit (4097) to give a bit of wiggle room for the encoding of roles.\n",
    "                # TODO: different limits for different models.\n",
    "\n",
    "                trimmed_prompt = limit_tokens_from_string(prompt, model, 4000 - max_tokens)\n",
    "\n",
    "                # Use chat completion API\n",
    "                messages = [{\"role\": \"system\" , \"content\": system_prompt},{\"role\": \"system\", \"content\": trimmed_prompt}]\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model='gpt-3.5-turbo',\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=2000,\n",
    "                    n=1,\n",
    "                    stop=None,\n",
    "                )    \n",
    "                return response.choices[0].message.content.strip()\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\n",
    "                \"   *** The OpenAI API rate limit has been exceeded. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.Timeout:\n",
    "            print(\n",
    "                \"   *** OpenAI API timeout occurred. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIError:\n",
    "            print(\n",
    "                \"   *** OpenAI API error occurred. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.APIConnectionError:\n",
    "            print(\n",
    "                \"   *** OpenAI API connection error occurred. Check your network settings, proxy configuration, SSL certificates, or firewall rules. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.InvalidRequestError:\n",
    "            print(\n",
    "                \"   *** OpenAI API invalid request. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        except openai.error.ServiceUnavailableError:\n",
    "            print(\n",
    "                \"   *** OpenAI API service unavailable. Waiting 10 seconds and trying again. ***\"\n",
    "            )\n",
    "            time.sleep(10)  # Wait 10 seconds and try again\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '..'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title_query_result = query_collection(\n",
    "    collection=question_collection,\n",
    "    query=question,\n",
    "    max_results=5,\n",
    "    dataframe=df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
