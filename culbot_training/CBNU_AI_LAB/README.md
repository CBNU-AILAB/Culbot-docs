# Cullbot(Chungbuk national university large language model-based chatbot)

## Example
![image](example.png)


## Foundation Model 

Cullbot ì€ Foudataion Modelë¡œ [Polyglot-ko-12.8B](https://huggingface.co/EleutherAI/polyglot-ko-12.8b)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

## Model repository

[yeongsang2/polyglot-ko-12.8B-v.1.02-checkpoint-4500](https://huggingface.co/yeongsang2/polyglot-ko-12.8B-v.1.02-checkpoint-4500)
<aside>
ğŸ’¡ í˜„ì¬ëŠ” ë³‘í•©ëœ ëª¨ë¸ì´ ì•„ë‹Œ adapterë¡œ ì œê³µë˜ê¸° ë•Œë¬¸ì—, inferenceì‹œ ë³‘í•©í•˜ë©´ ë©ë‹ˆë‹¤.
</aside>

## Dataset

ë°ì´í„°ì…‹ì€ ê³µê°œë˜ì–´ìˆëŠ” 1. [êµ¬ë¦„ ë°ì´í„°ì…‹](https://huggingface.co/datasets/nlpai-lab/kullm-v2)ê³¼ GPT3.5ë¡œ ìƒì„±í•´ë‚¸ ì¶©ë¶ëŒ€ ê´€ë ¨ 2. QA set 4k (data_cbnu.json) ì™€ 3. [ëŒ€í•™ë°±ê³¼](https://www.univ100.kr/)ì—ì„œ í¬ë¡¤ë§í•œ ë°ì´í„° 4K (data_crwaling)ë¥¼ í•©ì³ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤.

QA set ì€ ì¶©ë¶ëŒ€í•™êµ í™ˆí˜ì´ì§€ì— ìˆëŠ” ì •ë³´ê¸€ì„ ë°”íƒ•ìœ¼ë¡œ GPT3.5ê°€ "['instruction': ' ', 'input': '', 'output': '' ]" í˜•ì‹ë¡œ ìƒì„±í•´ëƒˆìœ¼ë©° ìì„¸í•œ ë‚´ìš©ì€ [notion](https://www.notion.so/CBNU-73265e0cb4b7491d92c063c637170b70?pvs=4)ê³¼ [https://github.com/yeongsang2/instruction_ai.git](https://github.com/yeongsang2/instruction_ai.git)ì— ì •ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

ëŒ€í•™ë°±ê³¼ í¬ë¡¤ë§ ê´€ë ¨ ì½”ë“œëŠ” [https://github.com/yeongsang2/crawling_univ](https://github.com/yeongsang2/crawling_univ)
ì—ì„œ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì—ëŠ” ì¶©ë¶ëŒ€ê´€ë ¨ QAë©´ tagë¥¼ 1ë¡œ, ì•„ë‹ˆë©´ tagë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í•™ìŠµì‹œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ê°€ ë“¤ì–´ê°€ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

### QAset ìƒì„± prompt
```
        You are an AI that generates data base information. 
        Create data of a specific structure based on the information I provide. 
        The information is as follows: 
        `
            {information}
        `
        The data structure is in the following JSON format. 
        `   
            {format}
        `
        Here are some examples of the data:
        `
            {example}
        `
        You need to comply with the following requirements.
        requirements:
        1. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.
        2. The content of the generated data should not be duplicated.
        3. All data (instruction, input, output) should be written in Korean.
        4. Create 10 pieces of data and arrange them in a list format.
        5. Please provide the answer without interruption and within the limited token range.
        6. Not all instructions require input. For example, when a instruction asks about some general information, "what is the highest peak in the world", it is not necssary to provide a specific context. In this case, we simply put "" in the input field.
        """
```
 
 ## Training with lora

Cullbotì€ Polyglot 12.8B ëª¨ë¸ì„ Low Rank Adaptation (LoRA)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.
ëª¨ë¸ í•™ìŠµì€ A100 80GB 4ëŒ€ë¡œ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. í•™ìŠµì— ì‚¬ìš©í•œ ì½”ë“œëŠ” [tloen/alpaca-lora](https://github.com/tloen/alpaca-lora)ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

### Denpendency
```
pip install -r requirements.txt
``` 

### training single gpu
train.sh

```
nohup python finetune_lora_cbnu.py \
    --base_model 'EleutherAI/polyglot-ko-12.8b' \
    --data_path '' \
    --output_dir output/ \
    --prompt_template_name cbnu2 \
    --batch_size 128 \
    --micro_batch_size 12 \
    --num_epochs 3 \
    --learning_rate 3e-4 \
	--optim "adamw_torch" \
    --cutoff_len 512 \
    --val_set_size 1000 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules "[query_key_value, xxx]" \
    --train_on_inputs True \
    --logging_steps 5 \
    --save_steps 100 \
    --eval_steps 100 \
    --warmup_steps 100 \
    --lr_scheduler_type "linear"  &
```

### training multi-gpu 
train.sh
```
python -m torch.distributed.launch  --master_port=34322 --nproc_per_node $gpu_num finetune_lora_cbnu.py \
    --base_model 'EleutherAI/polyglot-ko-12.8b' \
    --data_path "" \
    --output_dir output/ \
    --prompt_template_name cbnu2 \
    --batch_size 128 \
    --micro_batch_size 12 \
    --num_epochs 3 \
    --learning_rate 3e-4 \
	--optim "adamw_torch" \
    --cutoff_len 512 \
    --val_set_size 2000 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules "[query_key_value, xxx]" \
    --train_on_inputs True \
    --logging_steps 10 \
    --save_steps 1000 \
    --eval_steps 100 \
    --lr_scheduler_type "linear" \
	--warmup_steps 200 
```

### ì‹¤í–‰
```
sh train.sh
```

## Demo

demo.py
```
if __name__ == "__main__":
    gc.collect()
    torch.cuda.empty_cache()

    MODEL = "EleutherAI/polyglot-ko-12.8b"
    LORA_WEIGHTS = "yeongsang2/polyglot-ko-12.8B-v.1.02-checkpoint-4500"

    model = AutoModelForCausalLM.from_pretrained(MODEL, load_in_8bit=True,device_map={"":0})
    model = PeftModel.from_pretrained(model, LORA_WEIGHTS)
    model.eval()

    pipe = pipeline("text-generation", model=model, tokenizer=MODEL)
    prompter = Prompter("cbnu2")

    demo.launch(server_name="0.0.0.0", server_port=5000)
```
### ì‹¤í–‰
```
    python demo.py
```
<hr>