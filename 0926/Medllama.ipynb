{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeimWmMPDNGq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "Hm9BfLsjDVq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "OPENAI_API_KEY = \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "if OPENAI_API_KEY is not None:\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "    print (\"OPENAI_API_KEY is ready\")\n",
        "else:\n",
        "    print (\"OPENAI_API_KEY environment variable not found\")"
      ],
      "metadata": {
        "id": "Qz8jYuB8DVm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken as tiktoken\n",
        "import time\n",
        "\n",
        "\n",
        "def limit_tokens_from_string(string: str, model: str, limit: int) -> str:\n",
        "    \"\"\"Limits the string to a number of tokens (estimated).\"\"\"\n",
        "\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except:\n",
        "        encoding = tiktoken.encoding_for_model('gpt2')  # Fallback for others.\n",
        "\n",
        "    encoded = encoding.encode(string)\n",
        "\n",
        "    return encoding.decode(encoded[:limit])"
      ],
      "metadata": {
        "id": "hSnrGal9KdOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "OPENAI_TEMPERATURE = float(0.2)"
      ],
      "metadata": {
        "id": "I66xTB82DVlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def openai_call(\n",
        "    system_prompt: str,\n",
        "    prompt: str,\n",
        "    model: str = LLM_MODEL,\n",
        "    temperature: float = OPENAI_TEMPERATURE,\n",
        "    max_tokens: int = 100,\n",
        "):\n",
        "    while True:\n",
        "        try:\n",
        "            if not model.lower().startswith(\"gpt-\"):\n",
        "                # Use completion API\n",
        "                response = openai.Completion.create(\n",
        "                    engine=model,\n",
        "                    prompt=prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    top_p=1,\n",
        "                    frequency_penalty=0,\n",
        "                    presence_penalty=0,\n",
        "                )\n",
        "                return response.choices[0].text.strip()\n",
        "            else:\n",
        "                # Use 4000 instead of the real limit (4097) to give a bit of wiggle room for the encoding of roles.\n",
        "                # TODO: different limits for different models.\n",
        "\n",
        "                trimmed_prompt = limit_tokens_from_string(prompt, model, 4000 - max_tokens)\n",
        "\n",
        "                # Use chat completion API\n",
        "                messages = [{\"role\": \"system\" , \"content\": system_prompt},{\"role\": \"system\", \"content\": trimmed_prompt}]\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model='gpt-3.5-turbo',\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=2000,\n",
        "                    n=1,\n",
        "                    stop=None,\n",
        "                )\n",
        "                return response.choices[0].message.content.strip()\n",
        "        except openai.error.RateLimitError:\n",
        "            print(\n",
        "                \"   *** The OpenAI API rate limit has been exceeded. Waiting 10 seconds and trying again. ***\"\n",
        "            )\n",
        "            time.sleep(10)  # Wait 10 seconds and try again\n",
        "        except openai.error.Timeout:\n",
        "            print(\n",
        "                \"   *** OpenAI API timeout occurred. Waiting 10 seconds and trying again. ***\"\n",
        "            )\n",
        "            time.sleep(10)  # Wait 10 seconds and try again\n",
        "        except openai.error.APIError:\n",
        "            print(\n",
        "                \"   *** OpenAI API error occurred. Waiting 10 seconds and trying again. ***\"\n",
        "            )\n",
        "            time.sleep(10)  # Wait 10 seconds and try again\n",
        "        except openai.error.APIConnectionError:\n",
        "            print(\n",
        "                \"   *** OpenAI API connection error occurred. Check your network settings, proxy configuration, SSL certificates, or firewall rules. Waiting 10 seconds and trying again. ***\"\n",
        "            )\n",
        "            time.sleep(10)  # Wait 10 seconds and try again\n",
        "        except openai.error.InvalidRequestError:\n",
        "            print(\n",
        "                \"   *** OpenAI API invalid request. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. Waiting 10 seconds and trying again. ***\"\n",
        "            )\n",
        "            time.sleep(10)  # Wait 10 seconds and try again\n",
        "        except openai.error.ServiceUnavailableError:\n",
        "            print(\n",
        "                \"   *** OpenAI API service unavailable. Waiting 10 seconds and trying again. ***\"\n",
        "            )\n",
        "            time.sleep(10)  # Wait 10 seconds and try again\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "vet32ROzDVik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch transformers tokenizers accelerate"
      ],
      "metadata": {
        "id": "qCqYvEa8DVgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "import json"
      ],
      "metadata": {
        "id": "e9nplQCVDVeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'llSourcell/medllama2_7b'\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(MODEL)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "Hc20IadUDVcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON 파일 경로 설정\n",
        "json_file_path = '/content/gdrive/My Drive/Colab Notebooks/lab/data/qafilejson_4.json'\n",
        "\n",
        "# JSON 파일 읽기\n",
        "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)"
      ],
      "metadata": {
        "id": "LPejwtsqDVZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복문을 통해 instruction을 가져와서 언어모델을 실행\n",
        "jsonl_file_path = '/content/gdrive/My Drive/Colab Notebooks/lab/data/output.json'\n",
        "\n",
        "for item in data:\n",
        "    prompt = item[\"prompt\"]\n",
        "\n",
        "\n",
        "    # 영어로 번역해서 질문하기\n",
        "    system_prompt1 = \"You are an agent that generically translate incoming prompts as English prompts.\"\n",
        "    prompt1 = f\"\"\"\n",
        "        Please modify the following question into a translated one.\n",
        "        Here is an example.\n",
        "\n",
        "        ```\n",
        "        Question: 전립선암에 걸리면 소변을 볼 때 어떤 문제가 있나요?\n",
        "        After : What are the problems with urinating if you have prostate cancer?\n",
        "        ```\n",
        "\n",
        "        Question: {prompt}\n",
        "        After:\n",
        "\n",
        "        \"\"\"\n",
        "    response1 = openai_call(system_prompt1, prompt1, max_tokens=200)\n",
        "    #print(response1)\n",
        "\n",
        "    # prompt에 번역된 reponse1을instruction에 기입\n",
        "    prompt2 = f\"\"\"\n",
        "    instruction: {response1}\n",
        "    input :\n",
        "    output:\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt2, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate\n",
        "    generate_ids = model.generate(inputs.input_ids, max_length=200)\n",
        "    result = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    with open(jsonl_file_path, 'a', encoding='utf-8') as jsonl_file:\n",
        "        jsonl_file.write(json.dumps(result, ensure_ascii=False) + '\\n')\n"
      ],
      "metadata": {
        "id": "aQEDw-2UDVXf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}